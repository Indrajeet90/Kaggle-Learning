{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Machine Learning- Advanced.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "[View in Colaboratory](https://colab.research.google.com/github/Indrajeet90/Kaggle-Learning/blob/master/Machine_Learning_Advanced.ipynb)"
      ]
    },
    {
      "metadata": {
        "id": "y23VqR6Gn-PV",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Handling Misssing Values  \n",
        "We will see an example for predicting housing prices from the Melbourne Housing data.Most libraries (including scikit-learn) will give an error if one tries to build a model using data with missing values. So we'll need to choose one of the strategies below.   \n",
        "   1) A Simple Option: Drop Columns with Missing Values  \n",
        "   2) A Better Option: Imputation   \n",
        "   3) An Extension To Imputation  \n",
        "\n",
        "Basic Problem Set-up"
      ]
    },
    {
      "metadata": {
        "id": "int9IzZXnvQf",
        "colab_type": "code",
        "colab": {
          "resources": {
            "http://localhost:8080/nbextensions/google.colab/files.js": {
              "data": "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",
              "ok": true,
              "headers": [
                [
                  "content-type",
                  "application/javascript"
                ]
              ],
              "status": 200,
              "status_text": ""
            }
          },
          "base_uri": "https://localhost:8080/",
          "height": 72
        },
        "outputId": "804bf08f-7ad1-4ddc-a953-ea1eeede454b"
      },
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "uploaded = files.upload()"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-87b9aada-82b1-4f0b-9997-74f52ff0f858\" name=\"files[]\" multiple disabled />\n",
              "     <output id=\"result-87b9aada-82b1-4f0b-9997-74f52ff0f858\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script src=\"/nbextensions/google.colab/files.js\"></script> "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Saving melb_data.csv to melb_data.csv\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "3nCi-paYrRGt",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Load data\n",
        "melb_data = pd.read_csv('melb_data.csv')\n",
        "\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.metrics import mean_absolute_error\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "melb_target = melb_data.Price\n",
        "melb_predictors = melb_data.drop(['Price'], axis=1)\n",
        "\n",
        "# For the sake of keeping the example simple, we'll use only numeric predictors. \n",
        "melb_numeric_predictors = melb_predictors.select_dtypes(exclude=['object'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "8hfqd05yE_tn",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**Create Function to Measure Quality of An Approach**   \n",
        "We will divide our data into training and test. Next, We've loaded a function score_dataset(X_train, X_test, y_train, y_test) to compare the quality of diffrent approaches to missing values. This function reports the out-of-sample MAE score from a RandomForest."
      ]
    },
    {
      "metadata": {
        "id": "TW858TWFFBYG",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Creating a function to calculate quality of missing value treatment methods\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.metrics import mean_absolute_error\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(melb_numeric_predictors, \n",
        "                                                    melb_target,\n",
        "                                                    train_size=0.7, \n",
        "                                                    test_size=0.3, \n",
        "                                                    random_state=0)\n",
        "\n",
        "def score_dataset(X_train, X_test, y_train, y_test):\n",
        "    model = RandomForestRegressor()\n",
        "    model.fit(X_train, y_train)\n",
        "    preds = model.predict(X_test)\n",
        "    return mean_absolute_error(y_test, preds)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "mP6PKAd7FdHu",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**1. Get Model Score from Dropping Columns with Missing Values**"
      ]
    },
    {
      "metadata": {
        "id": "UrG13fgdFdwn",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "d618b723-42a5-45cc-88ac-0d4b35913d45"
      },
      "cell_type": "code",
      "source": [
        "cols_with_missing = [col for col in X_train.columns if X_train[col].isnull().any()]\n",
        "reduced_X_train = X_train.drop(cols_with_missing, axis=1)\n",
        "reduced_X_test  = X_test.drop(cols_with_missing, axis=1)\n",
        "print(\"Mean Absolute Error from dropping columns with Missing Values:\")\n",
        "print(score_dataset(reduced_X_train, reduced_X_test, y_train, y_test))"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mean Absolute Error from dropping columns with Missing Values:\n",
            "191151.5037344835\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "F-hIONG5FkkI",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**2. Get Model Score from Imputation**"
      ]
    },
    {
      "metadata": {
        "id": "SnhbbB9eFlN1",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "1f646c98-f8e3-4a7a-bc66-d29355375ba8"
      },
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from sklearn.preprocessing import Imputer\n",
        "my_imputer = Imputer()\n",
        "imputed_X_train = my_imputer.fit_transform(X_train)\n",
        "imputed_X_test = my_imputer.transform(X_test)\n",
        "print(\"Mean Absolute Error from Imputation:\")\n",
        "print(score_dataset(imputed_X_train, imputed_X_test, y_train, y_test))"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mean Absolute Error from Imputation:\n",
            "186056.3311160203\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "c1KZeyG4IT75",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**3.Get Score from Imputation with Extra Columns Showing What Was Imputed**"
      ]
    },
    {
      "metadata": {
        "id": "JlVJs4y_IUwi",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "df2e7d3f-dcfe-4389-aa9b-2ac75f3ba073"
      },
      "cell_type": "code",
      "source": [
        "imputed_X_train_plus = X_train.copy()\n",
        "imputed_X_test_plus = X_test.copy()\n",
        "\n",
        "cols_with_missing = (col for col in X_train.columns if X_train[col].isnull().any())\n",
        "for col in cols_with_missing:\n",
        "    imputed_X_train_plus[col + '_was_missing'] = imputed_X_train_plus[col].isnull()\n",
        "    imputed_X_test_plus[col + '_was_missing'] = imputed_X_test_plus[col].isnull()\n",
        "\n",
        "# Imputation\n",
        "my_imputer = Imputer()\n",
        "imputed_X_train_plus = my_imputer.fit_transform(imputed_X_train_plus)\n",
        "imputed_X_test_plus = my_imputer.transform(imputed_X_test_plus)\n",
        "\n",
        "print(\"Mean Absolute Error from Imputation while Track What Was Imputed:\")\n",
        "print(score_dataset(imputed_X_train_plus, imputed_X_test_plus, y_train, y_test))"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mean Absolute Error from Imputation while Track What Was Imputed:\n",
            "186479.5960294083\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "DL4zYd2wIZdJ",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "As is common, imputing missing values allowed us to improve our model compared to dropping those columns. We got an additional boost by tracking what values had been imputed."
      ]
    },
    {
      "metadata": {
        "id": "fRZBtR28Idga",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#Using Categorical Data"
      ]
    },
    {
      "metadata": {
        "id": "R6LRMBt2Ifgq",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "We will get an error trying to plug these variables into most machine learning models in Python without \"encoding\" them first. Here is the most popular method for encoding categorical variables.\n",
        "\n",
        "**One-Hot Encoding : The Standard Approach for Categorical Data**   \n",
        "One hot encoding is the most widespread approach, and it works very well unless your categorical variable takes on a large number of values. It creates new (binary) columns, indicating the presence of each possible value from the original data. Let's work through an example.    \n",
        "The values in the original data are Red, Yellow and Green. We create a separate column for each possible value. Wherever the original value was Red, we put a 1 in the Red column.\n"
      ]
    },
    {
      "metadata": {
        "id": "Hrl68-f8KBoN",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**Example of hot coding**   \n",
        "This data contains housing characteristics. You will use them to predict home prices, which are stored in a Series called target."
      ]
    },
    {
      "metadata": {
        "id": "8No2w0OTIxt_",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Read the data\n",
        "import pandas as pd\n",
        "train_data = pd.read_csv('train.csv')\n",
        "test_data = pd.read_csv('test.csv')\n",
        "\n",
        "# Drop houses where the target is missing\n",
        "train_data.dropna(axis=0, subset=['SalePrice'], inplace=True)\n",
        "\n",
        "target = train_data.SalePrice\n",
        "\n",
        "# Since missing values isn't the focus of this tutorial, we use the simplest\n",
        "# possible approach, which drops these columns. \n",
        "cols_with_missing = [col for col in train_data.columns if train_data[col].isnull().any()]                                  \n",
        "candidate_train_predictors = train_data.drop(['Id', 'SalePrice'] + cols_with_missing, axis=1)\n",
        "candidate_test_predictors = test_data.drop(['Id'] + cols_with_missing, axis=1)\n",
        "# \"cardinality\" means the number of unique values in a column.\n",
        "# We use it as our only way to select categorical columns here. This is convenient, though a little arbitrary.\n",
        "low_cardinality_cols = [cname for cname in candidate_train_predictors.columns if \n",
        "                                candidate_train_predictors[cname].nunique() < 10 and\n",
        "                                candidate_train_predictors[cname].dtype == \"object\"]\n",
        "numeric_cols = [cname for cname in candidate_train_predictors.columns if \n",
        "                                candidate_train_predictors[cname].dtype in ['int64', 'float64']]\n",
        "my_cols = low_cardinality_cols + numeric_cols\n",
        "train_predictors = candidate_train_predictors[my_cols]\n",
        "test_predictors = candidate_test_predictors[my_cols]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "cplJNzJDKQbn",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Pandas assigns a data type (called a dtype) to each column or Series. Let's see a random sample of dtypes from our prediction data:"
      ]
    },
    {
      "metadata": {
        "id": "gjyZ5OLnKNXQ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "outputId": "f0e431d5-27ad-41d7-c2c7-a9a7348c76ea"
      },
      "cell_type": "code",
      "source": [
        "train_predictors.dtypes.sample(10)"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Foundation    object\n",
              "GrLivArea      int64\n",
              "MSZoning      object\n",
              "YearBuilt      int64\n",
              "PoolArea       int64\n",
              "MSSubClass     int64\n",
              "Utilities     object\n",
              "CentralAir    object\n",
              "LandSlope     object\n",
              "ExterQual     object\n",
              "dtype: object"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 26
        }
      ]
    },
    {
      "metadata": {
        "id": "5Y-uGThmKXut",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Object indicates a column has text (there are other things it could be theoretically be, but that's unimportant for our purposes). It's most common to one-hot encode these \"object\" columns, since they can't be plugged directly into most models. Pandas offers a convenient function called get_dummies to get one-hot encodings. Call it like this:"
      ]
    },
    {
      "metadata": {
        "id": "v456VwEkKY4u",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "one_hot_encoded_training_predictors = pd.get_dummies(train_predictors)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "eeesM3_sKd03",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "One-hot encoding usually helps, but it varies on a case-by-case basis. In this case, there doesn't appear to be any meaningful benefit from using the one-hot encoded variables."
      ]
    },
    {
      "metadata": {
        "id": "4V3SA5muKb3g",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "8076a52d-87b5-46c7-85ed-b7204ebe8da4"
      },
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "\n",
        "def get_mae(X, y):\n",
        "    # multiple by -1 to make positive MAE score instead of neg value returned as sklearn convention\n",
        "    return -1 * cross_val_score(RandomForestRegressor(50), X, y, scoring = 'neg_mean_absolute_error').mean()\n",
        "\n",
        "predictors_without_categoricals = train_predictors.select_dtypes(exclude=['object'])\n",
        "\n",
        "mae_without_categoricals = get_mae(predictors_without_categoricals, target)\n",
        "\n",
        "mae_one_hot_encoded = get_mae(one_hot_encoded_training_predictors, target)\n",
        "\n",
        "print('Mean Absolute Error when Dropping Categoricals:',mae_without_categoricals)\n",
        "print('Mean Abslute Error with One-Hot Encoding:',mae_one_hot_encoded)"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mean Absolute Error when Dropping Categoricals: 18258.72892232334\n",
            "Mean Abslute Error with One-Hot Encoding: 18120.8573799444\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "TaSTE-YxKwxK",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# XGBoost  \n",
        "XGBoost is an implementation of the Gradient Boosted Decision Trees algorithm (scikit-learn has another version of this algorithm, but XGBoost has some technical advantages.) What is Gradient Boosted Decision Trees? We'll walk through a diagram.   \n",
        "![alt text](https://i.imgur.com/e7MIgXk.png)       \n",
        "We go through cycles that repeatedly builds new models and combines them into an ensemble model. We start the cycle by calculating the errors for each observation in the dataset. We then build a new model to predict those. We add predictions from this error-predicting model to the \"ensemble of models.\"    \n",
        "\n",
        "To make a prediction, we add the predictions from all previous models. We can use these predictions to calculate new errors, build the next model, and add it to the ensemble.    \n",
        "\n",
        "There's one piece outside that cycle. We need some base prediction to start the cycle. In practice, the initial predictions can be pretty naive. Even if it's predictions are wildly inaccurate, subsequent additions to the ensemble will address those errors."
      ]
    },
    {
      "metadata": {
        "id": "uRhSWhahLQ8V",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "We will start with the data pre-loaded into train_X, test_X, train_y, test_y."
      ]
    },
    {
      "metadata": {
        "id": "MSWs36J-K0iQ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import Imputer\n",
        "\n",
        "data = pd.read_csv('train.csv')\n",
        "data.dropna(axis=0, subset=['SalePrice'], inplace=True)\n",
        "y = data.SalePrice\n",
        "X = data.drop(['SalePrice'], axis=1).select_dtypes(exclude=['object'])\n",
        "train_X, test_X, train_y, test_y = train_test_split(X.as_matrix(), y.as_matrix(), test_size=0.25)\n",
        "\n",
        "my_imputer = Imputer()\n",
        "train_X = my_imputer.fit_transform(train_X)\n",
        "test_X = my_imputer.transform(test_X)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "dx2e7oUcLZm8",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**XGBoost Model:** We build and fit a model just as we would in scikit-learn."
      ]
    },
    {
      "metadata": {
        "id": "CRvvz4fHLV6f",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        },
        "outputId": "fb7b5104-8914-445a-e77a-308b5f6bc2be"
      },
      "cell_type": "code",
      "source": [
        "from xgboost import XGBRegressor\n",
        "\n",
        "my_model = XGBRegressor()\n",
        "# Add silent=True to avoid printing out updates with each cycle\n",
        "my_model.fit(train_X, train_y, verbose=False)"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
              "       colsample_bytree=1, gamma=0, learning_rate=0.1, max_delta_step=0,\n",
              "       max_depth=3, min_child_weight=1, missing=None, n_estimators=100,\n",
              "       n_jobs=1, nthread=None, objective='reg:linear', random_state=0,\n",
              "       reg_alpha=0, reg_lambda=1, scale_pos_weight=1, seed=None,\n",
              "       silent=True, subsample=1)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 30
        }
      ]
    },
    {
      "metadata": {
        "id": "tuXmn6Z_LjVZ",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "We similarly evaluate a model and make predictions as we would do in scikit-learn."
      ]
    },
    {
      "metadata": {
        "id": "f7tBNVjALpIk",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "99b18d6d-9ea4-471f-e9de-92b1a6c4cb14"
      },
      "cell_type": "code",
      "source": [
        "# make predictions\n",
        "predictions = my_model.predict(test_X)\n",
        "\n",
        "from sklearn.metrics import mean_absolute_error\n",
        "print(\"Mean Absolute Error : \",mean_absolute_error(predictions, test_y))"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mean Absolute Error :  19783.78955479452\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "ekKje4PIL8Uk",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#Model Tuning\n",
        "XGBoost has a few parameters that can dramatically affect your model's accuracy and training speed. The first parameters you should understand are:"
      ]
    },
    {
      "metadata": {
        "id": "Pm9XUdVHL-qQ",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**n_estimators and early_stopping_rounds**   \n",
        "**n_estimators** specifies how many times to go through the modeling cycle described above.The argument early_stopping_rounds offers a way to automatically find the ideal value. **Early stopping **causes the model to stop iterating when the validation score stops improving, even if we aren't at the hard stop for n_estimators. It's smart to set a high value for **n_estimators** and then use **early_stopping_rounds** to find the optimal time to stop iterating."
      ]
    },
    {
      "metadata": {
        "id": "ehUaCHR5MGP0",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Since random chance sometimes causes a single round where validation scores don't improve, you need to specify a number for how many rounds of straight deterioration to allow before stopping. **early_stopping_rounds = 5** is a reasonable value. Thus we stop after 5 straight rounds of deteriorating validation scores."
      ]
    },
    {
      "metadata": {
        "id": "7VnQ9NlNZsfc",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**Learning_rate**  \n",
        "Instead of getting predictions by simply adding up the predictions from each component model, we will multiply the predictions from each model by a small number before adding them in. This means each tree we add to the ensemble helps us less. In practice, this reduces the model's propensity to overfit.\n",
        "\n",
        "So, we can use a higher value of n_estimators without overfitting. If we use early stopping, the appropriate number of trees will be set automatically.\n",
        "\n",
        "In general, a small learning rate will yield more accurate XGBoost models, though it will also take the model longer to train since it does more iterations through the cycle."
      ]
    },
    {
      "metadata": {
        "id": "K71AVIvcM3w0",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        },
        "outputId": "7127cf64-7ae6-443b-dd82-2c16081c3bdb"
      },
      "cell_type": "code",
      "source": [
        "my_model = XGBRegressor(n_estimators=1000, learning_rate=0.05)\n",
        "my_model.fit(train_X, train_y, early_stopping_rounds=5, \n",
        "             eval_set=[(test_X, test_y)], verbose=False)"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
              "       colsample_bytree=1, gamma=0, learning_rate=0.05, max_delta_step=0,\n",
              "       max_depth=3, min_child_weight=1, missing=None, n_estimators=1000,\n",
              "       n_jobs=1, nthread=None, objective='reg:linear', random_state=0,\n",
              "       reg_alpha=0, reg_lambda=1, scale_pos_weight=1, seed=None,\n",
              "       silent=True, subsample=1)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 33
        }
      ]
    },
    {
      "metadata": {
        "id": "WypkcNblaXjO",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**n_jobs**   \n",
        "On larger datasets where runtime is a consideration, we can use parallelism to build your models faster. It's common to set the parameter n_jobs equal to the number of cores on a machine. On smaller datasets, this won't help."
      ]
    },
    {
      "metadata": {
        "id": "8UPS8qgcawUP",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Partial Dependence Plots"
      ]
    },
    {
      "metadata": {
        "id": "n2MH-oQ3ayG-",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Some people will argue we cannot see how machine learning models are working on any given dataset, so we can neither extract insight nor identify problems with the model.   \n",
        "\n",
        "Partial dependence plots show how each variable or predictor affects the model's predictions. This is useful for questions like:\n",
        "\n",
        "\n",
        "\n",
        "*  How much of wage differences between men and women are due solely to gender, as opposed to differences in education backgrounds or work experience?\n",
        "*   Controlling for house characteristics, what impact do longitude and latitude have on home prices?To restate this, we want to understand how similarly sized houses would be priced in different areas, even if the homes actually at these sites are different sizes. \n",
        "*   Are health differences between two groups due to differences in their diets, or due to other factors?\n",
        "\n",
        "Partial dependence plots can be interepreted similarly to the coefficients in linear or logistics models.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "qLsaaJNMbgAE",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Interpreting Partial Dependence Plots"
      ]
    },
    {
      "metadata": {
        "id": "A-538gnvbhdz",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "We'll start with 2 partial dependence plots showing the relationship between Price and a couple variables from the Melbourne Housing dataset. We'll walk through how these plots are created and interpreted."
      ]
    },
    {
      "metadata": {
        "id": "G3CoSgeobryj",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 280
        },
        "outputId": "66ed9ca1-10d6-4fb9-f22b-5039a46456f4"
      },
      "cell_type": "code",
      "source": [
        "# XGBoost model using Melbourne housing data\n",
        "import pandas as pd\n",
        "from sklearn.ensemble import GradientBoostingRegressor, GradientBoostingClassifier\n",
        "from sklearn.ensemble.partial_dependence import partial_dependence, plot_partial_dependence\n",
        "from sklearn.preprocessing import Imputer\n",
        "\n",
        "cols_to_use = ['Distance', 'Landsize', 'BuildingArea']\n",
        "\n",
        "# function to read data, impute missing values for selected columns\n",
        "def get_some_data():\n",
        "    data = pd.read_csv('melb_data.csv')\n",
        "    y = data.Price\n",
        "    X = data[cols_to_use]\n",
        "    my_imputer = Imputer()\n",
        "    imputed_X = my_imputer.fit_transform(X)\n",
        "    return imputed_X, y\n",
        "\n",
        "# get_some_data is defined above.\n",
        "X, y = get_some_data()\n",
        "# scikit-learn originally implemented partial dependence plots only for Gradient Boosting models\n",
        "# this was due to an implementation detail, and a future release will support all model types.\n",
        "my_model = GradientBoostingRegressor()\n",
        "# fit the model as usual\n",
        "my_model.fit(X, y)\n",
        "# Here we make the plot\n",
        "my_plots = plot_partial_dependence(my_model,       \n",
        "                                   features=[0, 2], # column numbers of plots we want to show\n",
        "                                   X=X,            # raw predictors data.\n",
        "                                   feature_names=['Distance', 'Landsize', 'BuildingArea'], # labels on graphs\n",
        "                                   grid_resolution=10) # number of values to plot on x axis"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAjQAAAEHCAYAAAC5jdydAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
            "text/plain": [
              "<matplotlib.figure.Figure at 0x7f58adf96dd8>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "metadata": {
        "id": "xYIxm9pmdIO5",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**The partial dependence plot is calculated only after the model has been fit. **The model is fit on real data. In that real data, houses in different parts of town may differ in myriad ways but after the model is fit, we could start by taking all the characteristics of a single house. Say, a house with 2 bedrooms, 2 bathrooms, a large lot, an age of 10 years, etc.      \n",
        "\n",
        "We then use the model to predict the price of that house, but we change the distance variable before making a prediction. We first predict the price for that house when sitting distance to 4. We then predict it's price setting distance to 5. Then predict again for 6. And so on. We trace out how predicted price changes (on the vertical axis) as we move from small values of distance to large values (on the horizontal axis).   \n",
        "\n",
        "Because of interactions, the partial dependence plot for a single house may be atypical. So, instead we repeat that mental experiment with multiple houses, and we plot the average predicted price on the vertical axis. You'll see some negative numbers. That doesn't mean the price would sell for a negative price. Instead it means the prices would have been less than the actual average price for that distance.       \n",
        "\n",
        "In the left graph, we see house prices fall as we get further from the central business distract. Though there seems to be a nice suburb about 16 kilometers out, where home prices are higher than many nearer and further suburbs.\n",
        "\n",
        "The right graph shows the impact of building area, which is interpreted similarly. A larger building area means higher prices."
      ]
    },
    {
      "metadata": {
        "id": "FDYscQiQfKnb",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Partial dependence plots are a great way (though not the only way) to extract insights from complex models. These can be incredibly powerful for communicating those insights to colleagues or non-technical users."
      ]
    },
    {
      "metadata": {
        "id": "5UYZOjjrgUsh",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Pipelines"
      ]
    },
    {
      "metadata": {
        "id": "yTxSjbICgWHT",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "A pipeline bundles preprocessing and modeling steps so we can use the whole bundle as if it were a single step.\n",
        "\n",
        "Many data scientists hack together models without pipelines, but Pipelines have some important benefits. Those include:\n",
        "\n",
        "*  **Cleaner Code:** We won't need to keep track of your training (and testing) data at each step of processing. Accounting for data at each step of processing can get messy. With a pipeline,we don't need to manually keep track of each step.\n",
        "*  **Fewer Bugs:** There are fewer opportunities to mis-apply a step or forget a pre-processing step.\n",
        "*  **Easier to Productionize:** It can be surprisingly hard to transition a model from a prototype to something deployable at scale. We won't go into the many related concerns here, but pipelines can help.\n",
        "*   **More Options For Model Testing:** We will see an example in the next tutorial, which covers cross-validation.\n",
        "\n",
        "We will use Melbourne dataset for our exercise.\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "nG8rnX3Bgpcw",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Read Data\n",
        "data = pd.read_csv('melb_data.csv')\n",
        "cols_to_use = ['Rooms', 'Distance', 'Landsize', 'BuildingArea', 'YearBuilt']\n",
        "X = data[cols_to_use]\n",
        "y = data.Price\n",
        "train_X, test_X, train_y, test_y = train_test_split(X, y)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "4Or6TBkdhRNW",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "We have a modeling process that uses an Imputer to fill in missing values, followed by a RandomForestRegressor to make predictions. These can be bundled together with the **make_pipeline** function as shown below."
      ]
    },
    {
      "metadata": {
        "id": "SSrZVk1yhM8C",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.pipeline import make_pipeline\n",
        "from sklearn.preprocessing import Imputer\n",
        "\n",
        "my_pipeline = make_pipeline(Imputer(), RandomForestRegressor())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "XHwzbRrGhti_",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "We can now fit and predict using this pipeline as a fused whole."
      ]
    },
    {
      "metadata": {
        "id": "RT4Qma_HhvQs",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "my_pipeline.fit(train_X, train_y)\n",
        "predictions = my_pipeline.predict(test_X)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "qeVNi0Aeh05N",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "For comparison, here is the code to do the same thing without pipelines"
      ]
    },
    {
      "metadata": {
        "id": "THEJC0Ifh1oV",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "my_imputer = Imputer()\n",
        "my_model = RandomForestRegressor()\n",
        "\n",
        "imputed_train_X = my_imputer.fit_transform(train_X)\n",
        "imputed_test_X = my_imputer.transform(test_X)\n",
        "my_model.fit(imputed_train_X, train_y)\n",
        "predictions = my_model.predict(imputed_test_X)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "XA7kb6IJiRb-",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Most scikit-learn objects are either transformers or models.\n",
        "\n",
        "**Transformers **are for pre-processing before modeling. The Imputer class (for filling in missing values) is an example of a transformer. Over time, we will learn many more transformers, and you will frequently use multiple transformers sequentially.\n",
        "\n",
        "**Models** are used to make predictions. We will usually preprocess your data (with transformers) before putting it in a model.\n",
        "\n",
        "We can tell if an object is a transformer or a model by how we apply it. After fitting a transformer, we apply it with the transform command. After fitting a model, we apply it with the predict command. A pipeline must start with transformer steps and end with a model. This is what we want anyway."
      ]
    },
    {
      "metadata": {
        "id": "y-usNj59jUKi",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Cross Validation\n",
        "Machine learning is an iterative process.\n",
        "\n",
        "We will face choices about predictive variables to use, what types of models to use,what arguments to supply those models, etc. We make these choices in a data-driven way by measuring model quality of various alternatives.\n",
        "\n",
        "You've already learned to use train_test_split to split the data, so you can measure model quality on the test data. Cross-validation extends this approach to model scoring (or \"model validation.\") Compared to train_test_split, cross-validation gives you a more reliable measure of your model's quality, though it takes longer to run.\n",
        "\n",
        "# The Shortcoming of Train-Test Split\n",
        "Imagine  a dataset with 5000 rows. The train_test_split function has an argument for test_size that  can be used to decide how many rows go to the training set and how many go to the test set. The larger the test set, the more reliable our measures of model quality will be. At an extreme, we could imagine having only 1 row of data in the test set. If you compare alternative models, which one makes the best predictions on a single data point will be mostly a matter of luck.\n",
        "\n",
        "But we can only get a large test set by removing data from our training data, and smaller training datasets mean worse models. In fact, the ideal modeling decisions on a small dataset typically aren't the best modeling decisions on large datasets."
      ]
    },
    {
      "metadata": {
        "id": "Gie8zt7lj6wk",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# The Cross-Validation Procedure\n",
        "In cross-validation, we run our modeling process on different subsets of the data to get multiple measures of model quality. For example, we could have 5 folds or experiments. We divide the data into 5 pieces, each being 20% of the full dataset.\n",
        "![alt text](https://i.stack.imgur.com/1fXzJ.png)\n",
        "We run an experiment called experiment 1 which uses the first fold as a holdout set, and everything else as training data. This gives us a measure of model quality based on a 20% holdout set, much as we got from using the simple train-test split.\n",
        "We then run a second experiment, where we hold out data from the second fold (using everything except the 2nd fold for training the model.) This gives us a second estimate of model quality. We repeat this process, using every fold once as the holdout. Putting this together, 100% of the data is used as a holdout at some point.\n",
        "\n",
        "Returning to our example above from train-test split, if we have 5000 rows of data, we end up with a measure of model quality based on 5000 rows of holdout (even if we don't use all 5000 rows simultaneously.\n",
        "\n",
        "# Trade-offs Between Cross-Validation and Train-Test Split\n",
        "Cross-validation gives a more accurate measure of model quality, which is especially important if you are making a lot of modeling decisions. However, it can take more time to run, because it estimates models once for each fold. So it is doing more total work.\n",
        "\n",
        "On small datasets, the extra computational burden of running cross-validation isn't a big deal.For the same reasons, a simple train-test split is sufficient for larger datasets.There's no simple threshold for what constitutes a large vs small dataset. Alternatively, we can run cross-validation and see if the scores for each experiment seem close. If each experiment gives the same results, train-test split is probably sufficient."
      ]
    },
    {
      "metadata": {
        "id": "ofex3oWDkuP7",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**Cross-validation** example using Melbourne data"
      ]
    },
    {
      "metadata": {
        "id": "ez32gbobk2Qm",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "data = pd.read_csv('melb_data.csv')\n",
        "cols_to_use = ['Rooms', 'Distance', 'Landsize', 'BuildingArea', 'YearBuilt']\n",
        "X = data[cols_to_use]\n",
        "y = data.Price"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Tb6LGkJclE4J",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Then specify a pipeline of our modeling steps."
      ]
    },
    {
      "metadata": {
        "id": "N9Jiacn8lAAC",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.pipeline import make_pipeline\n",
        "from sklearn.preprocessing import Imputer\n",
        "my_pipeline = make_pipeline(Imputer(), RandomForestRegressor())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "OKZRyXh4lTfN",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Finally get the cross-validation scores:"
      ]
    },
    {
      "metadata": {
        "id": "dCSjVOAmlU3f",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "44c1286f-5957-4996-f352-2524646fd666"
      },
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import cross_val_score\n",
        "scores = cross_val_score(my_pipeline, X, y, scoring='neg_mean_absolute_error')\n",
        "print(scores)"
      ],
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[-310659.07801954 -301505.90598152 -298776.70800048]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "j_1yhbBLlsId",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Scikit-learn has a convention where all metrics are defined so a high number is better. Using negatives here allows them to be consistent with that convention, though negative MAE is almost unheard of elsewhere.We typically want a single measure of model quality to compare between models. So we take the average across experiments."
      ]
    },
    {
      "metadata": {
        "id": "ZUC-dgT7l1tU",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "fd59b863-9c5f-472c-b24c-d4ed55ffae4d"
      },
      "cell_type": "code",
      "source": [
        "print('Mean Absolute Error:', (-1 * scores.mean()))"
      ],
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mean Absolute Error: 303647.23066717666\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "5EAH4uu3mfJt",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Data Leakage\n",
        " Data leakage causes a model to look accurate until one starts making decisions with the model, and then the model becomes very inaccurate. \n",
        "\n",
        "There are two main types of leakage: **Leaky Predictors** and a **Leaky Validation Strategies**."
      ]
    },
    {
      "metadata": {
        "id": "IiJtbfcMm5kt",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**Leaky Predictors**   \n",
        "This occurs when your predictors include data that will not be available at the time you make predictions.To prevent this type of data leakage, any variable updated (or created) after the target value is realized should be excluded. Because when we use this model to make new predictions, that data won't be available to the model."
      ]
    },
    {
      "metadata": {
        "id": "Lkjkql1tnFvB",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**Leaky Validation Strategy**   \n",
        "A much different type of leak occurs when one is not careful distinguishing training data from validation data. For example, this happens if one runs preprocessing (like fitting the Imputer for missing values) before calling train_test_split. Validation is meant to be a measure of how the model does on data it hasn't considered before. "
      ]
    },
    {
      "metadata": {
        "id": "GU0qOx8Qn2Ox",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Example\n",
        "We will use a small dataset about credit card applications, and we will build a model predicting which applications were accepted (stored in a variable called card). Here is a look at the data:"
      ]
    },
    {
      "metadata": {
        "id": "PSRhgj0il8IZ",
        "colab_type": "code",
        "colab": {
          "resources": {
            "http://localhost:8080/nbextensions/google.colab/files.js": {
              "data": "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",
              "ok": true,
              "headers": [
                [
                  "content-type",
                  "application/javascript"
                ]
              ],
              "status": 200,
              "status_text": ""
            }
          },
          "base_uri": "https://localhost:8080/",
          "height": 72
        },
        "outputId": "87dc514f-f85c-4389-fce2-25730bf51676"
      },
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "uploaded = files.upload()"
      ],
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-feb4b758-ca2b-456b-b627-5ffe89e57705\" name=\"files[]\" multiple disabled />\n",
              "     <output id=\"result-feb4b758-ca2b-456b-b627-5ffe89e57705\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script src=\"/nbextensions/google.colab/files.js\"></script> "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Saving AER_credit_card_data.csv to AER_credit_card_data.csv\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "yySBPjrToFGp",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 238
        },
        "outputId": "096c74eb-5076-4d8d-dc29-0fd6bbd44aa3"
      },
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "data = pd.read_csv('AER_credit_card_data.csv', true_values = ['yes'],false_values = ['no'])\n",
        "print(data.head())"
      ],
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "   card  reports       age  income     share  expenditure  owner  selfemp  \\\n",
            "0  True        0  37.66667  4.5200  0.033270   124.983300   True    False   \n",
            "1  True        0  33.25000  2.4200  0.005217     9.854167  False    False   \n",
            "2  True        0  33.66667  4.5000  0.004156    15.000000   True    False   \n",
            "3  True        0  30.50000  2.5400  0.065214   137.869200  False    False   \n",
            "4  True        0  32.16667  9.7867  0.067051   546.503300   True    False   \n",
            "\n",
            "   dependents  months  majorcards  active  \n",
            "0           3      54           1      12  \n",
            "1           3      34           1      13  \n",
            "2           4      58           1       5  \n",
            "3           0      25           1       7  \n",
            "4           2      64           1       5  \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "6PZAExqpoRdX",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "d1f3c230-13a5-45c6-a1f3-c921604b2b21"
      },
      "cell_type": "code",
      "source": [
        "from sklearn.pipeline import make_pipeline\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.model_selection import cross_val_score\n",
        "\n",
        "y = data.card\n",
        "X = data.drop(['card'], axis=1)\n",
        "\n",
        "# Since there was no preprocessing, we didn't need a pipeline here. Used anyway as best practice\n",
        "modeling_pipeline = make_pipeline(RandomForestClassifier())\n",
        "cv_scores = cross_val_score(modeling_pipeline, X, y, scoring='accuracy')\n",
        "print(\"Cross-val accuracy:\", cv_scores.mean())"
      ],
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cross-val accuracy: 0.9780130461793332\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "HgUMEdkgonnN",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "It's very rare to find models that are accurate 98% of the time. It happens, but it's rare enough that we should inspect the data more closely to see if it is target leakage.  \n",
        "Here is a summary of the data, which you can also find under the data tab:\n",
        "\n",
        "**card:** Dummy variable, 1 if application for credit card accepted, 0 if not  \n",
        "**reports:** Number of major derogatory reports  \n",
        "**age:** Age n years plus twelfths of a year  \n",
        "**income**: Yearly income (divided by 10,000)  \n",
        "**share:** Ratio of monthly credit card expenditure to yearly income  \n",
        "**expenditure**: Average monthly credit card expenditure  \n",
        "**owner: **1 if owns their home, 0 if rent  \n",
        "**selfempl: **1 if self employed, 0 if not.  \n",
        "**dependents: **1 + number of dependents  \n",
        "**months:** Months living at current address  \n",
        "**majorcards:** Number of major credit cards held  \n",
        "**active: **Number of active credit accounts  \n",
        "A few variables look suspicious. For example, does expenditure mean expenditure on this card or on cards used before appying?\n",
        "\n",
        "At this point, basic data comparisons can be very helpful:"
      ]
    },
    {
      "metadata": {
        "id": "op77co2EpCtN",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "d6c1272b-dc2a-46e7-aaf6-fbbf64ef84e1"
      },
      "cell_type": "code",
      "source": [
        "expenditures_cardholders = data.expenditure[data.card]\n",
        "expenditures_noncardholders = data.expenditure[~data.card]\n",
        "\n",
        "print('Fraction of those who received a card with no expenditures:',\\\n",
        "      ( expenditures_cardholders == 0).mean())\n",
        "print(\"Fraction of those who didn't receive a card with no expenditures:\", \\\n",
        "      (expenditures_noncardholders == 0).mean())"
      ],
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Fraction of those who received a card with no expenditures: 0.020527859237536656\n",
            "Fraction of those who didn't receive a card with no expenditures: 1.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "HBN7ItUKqKLS",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Since share is partially determined by expenditure, it should be excluded too. The variables active, majorcards are a little less clear, but from the description, they sound concerning. In most situations, it's better to be safe than sorry if you can't track down the people who created the data to find out more.\n",
        "\n",
        "We would run a model without leakage as follows:"
      ]
    },
    {
      "metadata": {
        "id": "Aucup03DpdGV",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "28576622-2d75-46ce-b558-e3ea913c8ff7"
      },
      "cell_type": "code",
      "source": [
        "potential_leaks = ['expenditure', 'share', 'active', 'majorcards']\n",
        "X2 = X.drop(potential_leaks, axis=1)\n",
        "cv_scores = cross_val_score(modeling_pipeline, X2, y, scoring='accuracy')\n",
        "print(\"Cross-val accuracy: %f\" %cv_scores.mean())"
      ],
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cross-val accuracy: 0.818042\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "dvc_bMOeqSLK",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "This accuracy is quite a bit lower, which on the one hand is disappointing. However, we can expect it to be right about 80% of the time when used on new applications, whereas the leaky model would likely do much worse then that (even in spite of it's higher apparent score in cross-validation.)."
      ]
    }
  ]
}